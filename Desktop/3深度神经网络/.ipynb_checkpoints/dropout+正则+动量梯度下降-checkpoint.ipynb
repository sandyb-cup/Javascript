{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.851618Z",
     "start_time": "2020-09-29T09:09:06.535591Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (5.0, 4.0)\n",
    "\n",
    "plt.rcParams[\"image.interpolation\"] = \"nearset\"\n",
    "\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.880948Z",
     "start_time": "2020-09-29T09:09:06.856606Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "train_x_orig, train_y_orig, test_x_orig, test_y_orig, classes = load_data()\n",
    "\n",
    "train_x = train_x_orig.reshape(train_x_orig.shape[0],-1).T/225\n",
    "\n",
    "train_y = train_y_orig\n",
    "\n",
    "test_x = test_x_orig.reshape(test_x_orig.shape[0], -1).T /225\n",
    "\n",
    "test_y = test_y_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.887039Z",
     "start_time": "2020-09-29T09:09:06.882758Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize(dim_info):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    parameters = {}\n",
    "    \n",
    "    dim_len = len(dim_info)\n",
    "    \n",
    "    for l in range(1, dim_len):\n",
    "        \n",
    "        parameters[\"W\"+str(l)] = np.random.randn(dim_info[l], dim_info[l-1])*np.sqrt(2/dim_info[l - 1]) \n",
    "        \n",
    "        \n",
    "        # 将w进行简单归一化处理\n",
    "        \n",
    "        parameters[\"b\"+str(l)] = np.zeros((dim_info[l], 1))\n",
    "        \n",
    "        \n",
    "    return parameters\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.892072Z",
     "start_time": "2020-09-29T09:09:06.889141Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def line_forward(A, W, b):\n",
    "\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    forward_Z_parameters = (A, W, b)\n",
    "    \n",
    "    return Z, forward_Z_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.897242Z",
     "start_time": "2020-09-29T09:09:06.893698Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def line_activate(A_prev, W, b, activation):\n",
    "    \n",
    "    Z, forward_Z_parameters = line_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    \n",
    "    else:\n",
    "        activation == \"rule\"\n",
    "        A = relu(Z)\n",
    "        \n",
    "    forward_activation_parameters = (forward_Z_parameters, Z)\n",
    "    \n",
    "    return A, forward_activation_parameters\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.909536Z",
     "start_time": "2020-09-29T09:09:06.899042Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def line_model(X, parameters):\n",
    "    \n",
    "    forward_model_cache = []\n",
    "    \n",
    "    A = X\n",
    "    \n",
    "    layer_deep = len(parameters)//2\n",
    "    \n",
    "    for l in range(1,layer_deep): #  对网络层进行遍历 \n",
    "        A_prev = A\n",
    "        \n",
    "        A, activation_parameters = line_activate(A_prev, parameters['W'+str(l)], parameters['b'+str(l)], activation = \"rule\")\n",
    "        # 输入层与隐藏层使用rule激活函数\n",
    "        \n",
    "        forward_model_cache.append(activation_parameters)\n",
    "        \n",
    "    Al, activation_parameters = line_activate(A, parameters[\"W\"+str(layer_deep)], parameters[\"b\"+str(layer_deep)], activation = 'sigmoid')\n",
    "    # 输出层的Al参数\n",
    "    \n",
    "    forward_model_cache.append(activation_parameters)\n",
    "    \n",
    "    return Al, forward_model_cache\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">正向传播使用droupout</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.929248Z",
     "start_time": "2020-09-29T09:09:06.922256Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_model_droupout(X, parameters, dropout_num, seed, keep_dropout):\n",
    "    \n",
    "    forward_model_cache = []\n",
    "    \n",
    "    random_dropout_cache = []\n",
    "    \n",
    "    layer_deep = len(parameters)//2\n",
    "    \n",
    "    A = X\n",
    "    \n",
    "    \n",
    "    for l in range(1, layer_deep):\n",
    "        A_prev = A\n",
    "        \n",
    "        A, activation_parameters = line_activate(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)],activation = 'rule')\n",
    "        \n",
    "        forward_model_cache.append(activation_parameters)\n",
    "        \n",
    "        if keep_dropout ==\"True\":\n",
    "\n",
    "            np.random.seed(seed+1)\n",
    "\n",
    "            random_del = np.random.rand(A.shape[0], A.shape[1])\n",
    "           \n",
    "            zero_random = random_del < dropout_num # 失活百分比\n",
    "            \n",
    "\n",
    "            result_num = A * zero_random\n",
    "\n",
    "            A = result_num / dropout_num # 使失活后的期望值与没失活之前的期望值保持一致\n",
    "            \n",
    "            random_dropout_cache.append(zero_random) # 将失活的比值储存 后面进行反向传播时再调用\n",
    "        else: \n",
    "            continue\n",
    "        \n",
    "    Al, activation_parameters = line_activate(A, parameters[\"W\"+str(layer_deep)], parameters[\"b\"+str(layer_deep)], activation = 'sigmoid')\n",
    "    \n",
    "    forward_model_cache.append(activation_parameters)\n",
    "    \n",
    "    # 输出层不使用droupout失活 \n",
    "    \n",
    "    return Al, forward_model_cache, random_dropout_cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.934324Z",
     "start_time": "2020-09-29T09:09:06.931067Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def computer_cost(Al, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = (-1/m ) * np.sum(np.multiply(Y, np.log(Al)) + np.multiply(1 - Y, np.log(1 - Al)))\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.940299Z",
     "start_time": "2020-09-29T09:09:06.935956Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def back_forward(dz, forward_Z_parameters):\n",
    "    \n",
    "    a_prev, W, b = forward_Z_parameters\n",
    "    \n",
    "    m = a_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dz, forward_Z_parameters[0].T)/m\n",
    "    \n",
    "    db = np.sum(dz, axis=1, keepdims = True )/ m\n",
    "    \n",
    "    dA = np.dot(forward_Z_parameters[1].T, dz)\n",
    "    \n",
    "    return dA, dW, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.945826Z",
     "start_time": "2020-09-29T09:09:06.942128Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def line_activation_backward(dA, forward_model_cache, activation):\n",
    "    \n",
    "    linear_caches, activation_cache = forward_model_cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dz = relu_backward(dA, activation_cache)\n",
    "        \n",
    "    else:\n",
    "        activation == \"sigmoid\"\n",
    "        dz = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    da_prev, dw, db = back_forward(dz, linear_caches)\n",
    "    # 根据这一步得出来的dz 算出上一网络层的偏导数da 与当前网络层的偏导数dw， db\n",
    "    \n",
    "    return da_prev, dw, db\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.953827Z",
     "start_time": "2020-09-29T09:09:06.947563Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def line_model_backward(Al, y, forward_model_cache ):\n",
    "    \n",
    "    grades = {}\n",
    "    \n",
    "    l = len(forward_model_cache)\n",
    "    \n",
    "    m = y.shape[1]\n",
    "    \n",
    "    y = y.reshape(Al.shape)\n",
    "    \n",
    "    dal = -(np.divide(y, Al) - np.divide(1-y, 1-Al))\n",
    "    \n",
    "    last_layer_parameters = forward_model_cache[-1]\n",
    "    \n",
    "    grades[\"dA\"+str(l-1)], grades[\"dW\"+str(l)], grades[\"db\"+str(l)] = line_activation_backward(dal, last_layer_parameters, activation='sigmoid')\n",
    "\n",
    "    # 根据da得出倒数第二层的dA,跟倒数第一层的dW，db\n",
    "    \n",
    "    \n",
    "    for i in reversed(range(1, l)):\n",
    "        grades[\"dA\"+str(i - 1)],grades['dW'+str(i)], grades[\"db\"+str(i)] = line_activation_backward(grades[\"dA\"+str(i)], forward_model_cache[i-1], activation = 'relu')\n",
    "        \n",
    "        \n",
    "    return grades\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\"># 反向失活函数</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.963665Z",
     "start_time": "2020-09-29T09:09:06.955684Z"
    }
   },
   "outputs": [],
   "source": [
    "def line_modle_backward_droupout(Al, y, forward_model_cache, dropout_cache, keep_dropout, drop_num = 0.5):\n",
    "    \n",
    "    grades = {}\n",
    "    \n",
    "    l = len(forward_model_cache)\n",
    "\n",
    "    m = y.shape[1]\n",
    "    \n",
    "    y = y.reshape(Al.shape)\n",
    "    \n",
    "    # dal = -(np.divide(y, Al)) - np.divide(1-y, 1-Al)\n",
    "    \n",
    "    dal = -(np.divide(y, Al) - np.divide(1-y, 1-Al))\n",
    "    \n",
    "    last_layer_parameters = forward_model_cache[-1]\n",
    "    \n",
    "    grades[\"dA\"+str(l - 1)], grades[\"dW\"+ str(l)], grades['db'+str(l)] = line_activation_backward(dal, last_layer_parameters, activation='sigmoid')\n",
    "    \n",
    "    # if keep_dropout ==\"True\":\n",
    "    \n",
    "        # grades[\"dA\"+str(l - 1)] = grades['dA'+str(l - 1)] * dropout_cache[l - 1]\n",
    "\n",
    "        # grades[\"dA\"+str(l - 1)] = grades[\"dA\"+str(l - 1)] / droup_num\n",
    "        \n",
    "        # 隐藏层最后一层梯度\n",
    "\n",
    "    for i in reversed(range(1, l)):\n",
    "        grades['dA'+ str(i - 1)], grades[\"dW\"+str(i)], grades[\"db\"+str(i)] = line_activation_backward(grades['dA'+ str(i)], forward_model_cache[i - 1], activation ='relu' )\n",
    "        \n",
    "        if keep_dropout ==\"True\" and i != 1: \n",
    "            # 这里只对隐藏层进行反向的dropout梯度计算 在前向传播时也只对隐藏层进行dropout\n",
    "        \n",
    "            grades['dA'+ str(i - 1)] = grades[\"dA\"+str(i - 1)] * dropout_cache[i - 2]\n",
    "\n",
    "            grades[\"dA\"+ str(i - 1)] = grades['dA'+str(i - 1)] / drop_num # 使期望值保持一致\n",
    "            \n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "     \n",
    "    return grades \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:06.969828Z",
     "start_time": "2020-09-29T09:09:06.965959Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def updata_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for i in range(1,L+1):\n",
    "        \n",
    "        parameters[\"W\"+ str(i)] = parameters[\"W\"+ str(i)] - grads['dW'+str(i)]*learning_rate\n",
    "        \n",
    "        \n",
    "        parameters[\"b\"+str(i)] = parameters[\"b\"+str(i)] - grads['db'+str(i)]*learning_rate\n",
    "    \n",
    "        \n",
    "    return parameters\n",
    "            \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span class=\"burk\">Aadm梯度下降</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:07.031984Z",
     "start_time": "2020-09-29T09:09:07.019272Z"
    }
   },
   "outputs": [],
   "source": [
    "def updata_parameters_Adam(parameters, grads, learning_rate, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8,  keep_Adam = \"True\"):\n",
    "    \n",
    "    # 这里使用Aadm梯度下降是要注意是对更新函数时的梯度进行Aadm修正\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    v_corrected = {}\n",
    "    \n",
    "    s_corrected = {}\n",
    "    \n",
    "    if keep_Adam == \"True\":\n",
    "    \n",
    "        for l in range(L):\n",
    "            print('Adam')\n",
    "            \n",
    "            v['dW'+str(l + 1)] = beta1 * grads[\"dW\"+str(l + 1)] + (1 - beta1) * grads[\"dW\"+str(l + 1)]\n",
    "            v['db'+str(l + 1)] = beta1 * grads[\"db\"+str(l + 1)] + (1 - beta1) * grads[\"db\"+str(l + 1)]\n",
    "            \n",
    "            v_corrected[\"dW\"+str(l + 1)] = v[\"dW\"+str(l + 1)] / np.power(1 - beta1, 2) # 修正函数\n",
    "            v_corrected[\"db\"+str(l + 1)] = v[\"dW\"+str(l + 1)] / np.power(1 - beta1, 2)\n",
    "            \n",
    "            s['dW'+str(l + 1)] = beta2 * grads['dW'+str(l + 1)] + (1 - beta2) * np.power(grads[\"dW\"+str(l + 1)], 2) # 进行RSM 后面与指数加平均的区别是后面是乘梯度的平方\n",
    "            s['db'+str(l + 1)] = beta2 * grads[\"db\"+str(l + 1)] + (1 - beta2) * np.power(grads[\"db\"+str(l + 1)], 2)\n",
    "            \n",
    "            s_corrected['dW'+str(l + 1)] = s[\"dW\"+str(l + 1)] / np.power(1 - beta2, 2)\n",
    "            s_corrected['db'+str(l + 1)] = s['db'+str(l + 1)] / np.power(1 - beta2, 2)\n",
    "            \n",
    "            parameters[\"W\"+str(l + 1)] = parameters['dW'+str(l + 1)] - learning_rate * v_corrected[\"dW\"+str(l + 1)] / np.sqrt(s_corrected[\"dW\"+str(l + 1)] + epsilon)\n",
    "            parameters['b'+str(l + 1)] = parameters[\"db\"+str(l + 1)] - learning_rate * v_corrected['db'+str(l + 1)] / np.sqrt(s_corrected[\"db\"+str(l + 1)] + epsilon)\n",
    "    else:\n",
    "        keep_Adam == 'False'\n",
    "        for l in range(L):\n",
    "            parameters[\"W\"+str(l + 1)] = parameters[\"W\"+str(l + 1)] - learning_rate * grads[\"dW\"+str(l + 1)]\n",
    "            parameters['b'+str(l + 1)] = parameters[\"b\"+str(l + 1)] - learning_rate * grads[\"db\"+str(l + 1)]\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:07.071306Z",
     "start_time": "2020-09-29T09:09:07.062823Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def dnn_model(x,\n",
    "              y,\n",
    "              dim_info,\n",
    "              learning_rate = 0.0075,\n",
    "              num_iterations = 100,\n",
    "              print_cost = True,\n",
    "              dropout_num = 1,\n",
    "              keep_dropout = False,\n",
    "              decay_rate = 0.5,\n",
    "              keep_Adam = \"True\",\n",
    "              beta1 = 0.9,\n",
    "              beta2 = 0.999,\n",
    "              epsilon = 1e-8):\n",
    "\n",
    "    # decay_rate 动量梯度下降超参数\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    m = y.shape[1]\n",
    "\n",
    "    parameters = initialize(dim_info)\n",
    "\n",
    "    seed = 0\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Al, forward_model_cache = line_model(x, parameters)\n",
    "        seed += 1\n",
    "\n",
    "        Al, forward_model_cache, dropout_cache = line_model_droupout(\n",
    "            x, parameters, dropout_num, seed, keep_dropout)\n",
    "\n",
    "        cost = computer_cost(Al, y)\n",
    "\n",
    "        # grades = line_model_backward(Al, y, forward_model_cache) # 进行反向传播\n",
    "\n",
    "        grades = line_modle_backward_droupout(Al, y, forward_model_cache,\n",
    "                                              dropout_cache, keep_dropout,\n",
    "                                              dropout_num)\n",
    "        \n",
    "        # 动态梯度下降\n",
    "\n",
    "        if i % 500 == 0:\n",
    "\n",
    "            epoch_num = i // 500\n",
    "            print('epoch_num:', epoch_num)\n",
    "\n",
    "            learning_rate = 1 / (1 + decay_rate * epoch_num) * learning_rate\n",
    "\n",
    "            print('learning_rate:', learning_rate)\n",
    "\n",
    "        # parameters = updata_parameters(parameters, grades, learning_rate)\n",
    "        \n",
    "        parameters = updata_parameters_Adam(parameters, grades, learning_rate, beta1 = 0.9, beta2 = 0.09, keep_Adam = \"False\", epsilon = 1e-8)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            if print_cost and i > 0:\n",
    "                print('训练%i次后的成本是: %f' % (i, cost))\n",
    "                # print(dropout_cache[1])\n",
    "            costs.append(cost)\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations(per tens)')\n",
    "    plt.title('Learning rate =' + str(learning_rate))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:18.220673Z",
     "start_time": "2020-09-29T09:09:07.096227Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_num: 0\n",
      "learning_rate: 0.0075\n",
      "训练100次后的成本是: 0.673286\n",
      "训练200次后的成本是: 0.651721\n",
      "训练300次后的成本是: 0.602577\n",
      "训练400次后的成本是: 0.587147\n",
      "epoch_num: 1\n",
      "learning_rate: 0.004999999999999999\n",
      "训练500次后的成本是: 0.516014\n",
      "训练600次后的成本是: 0.463296\n",
      "训练700次后的成本是: 0.441060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-34a35764ee6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0075\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"True\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_Adam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'True'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-18d4cd0dcadb>\u001b[0m in \u001b[0;36mdnn_model\u001b[0;34m(x, y, dim_info, learning_rate, num_iterations, print_cost, dropout_num, keep_dropout, decay_rate, keep_Adam, beta1, beta2, epsilon)\u001b[0m\n\u001b[1;32m     37\u001b[0m         grades = line_modle_backward_droupout(Al, y, forward_model_cache,\n\u001b[1;32m     38\u001b[0m                                               \u001b[0mdropout_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                                               dropout_num)\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# 动态梯度下降\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ac9256fbdbc3>\u001b[0m in \u001b[0;36mline_modle_backward_droupout\u001b[0;34m(Al, y, forward_model_cache, dropout_cache, keep_dropout, drop_num)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dA'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline_activation_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dA'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_model_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeep_dropout\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"True\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7036c78377e8>\u001b[0m in \u001b[0;36mline_activation_backward\u001b[0;34m(dA, forward_model_cache, activation)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mda_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mback_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_caches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 根据这一步得出来的dz 算出上一网络层的偏导数da 与当前网络层的偏导数dw， db\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5c9331433651>\u001b[0m in \u001b[0;36mback_forward\u001b[0;34m(dz, forward_Z_parameters)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_Z_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dim_info = [12288, 30, 8, 6, 1]\n",
    "\n",
    "ßß\n",
    "parameters = dnn_model(train_x, train_y, dim_info, learning_rate=0.0075, num_iterations = 4000, print_cost = True, dropout_num = 0.7, keep_dropout = \"True\", decay_rate = 0.5, beta1 = 0.9, beta2 = 0.99, keep_Adam = 'True', epsilon = 1e-8)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:18.273920Z",
     "start_time": "2020-09-29T09:09:07.123Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    n = len(parameters) //2\n",
    "    \n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    probas, caches = line_model(X, parameters) # 这里要记住在实际预测时要把droupout关闭在进行预测\n",
    "    \n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0, i] > 0.5:\n",
    "            p[0, i] = 1\n",
    "        else:\n",
    "            p[0, i] = 0\n",
    "            \n",
    "    return p\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:18.277319Z",
     "start_time": "2020-09-29T09:09:07.144Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_train = predict(train_x, parameters)\n",
    "\n",
    "print('预测集的准确率是:'+str(np.sum((pred_train == train_y) / train_x.shape[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T09:09:18.281895Z",
     "start_time": "2020-09-29T09:09:07.162Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_test = predict(test_x, parameters)\n",
    "\n",
    "print('测试集的准确率是:'+str(np.sum((pred_test == test_y) / test_x.shape[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
